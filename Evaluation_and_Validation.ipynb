{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "text_corpora = \"\"\"Machine learning is a subset of artificial intelligence (AI) that allows systems to learn and improve from experience without being explicitly programmed. It involves creating algorithms that can identify patterns in data and make decisions or predictions based on that data. The field has seen rapid growth in recent years, with applications ranging from speech recognition to self-driving cars.\n",
        "\n",
        "Types of Machine Learning\n",
        "\n",
        "Machine learning can be classified into three main types:\n",
        "\n",
        "Supervised Learning: In supervised learning, the algorithm is trained on labeled data. The training set includes input-output pairs, where the model learns to map inputs to the correct output. A common example is image classification, where the model is trained with images that are labeled as belonging to specific categories (e.g., cat, dog, etc.). The goal is to learn a mapping that can be used to predict labels for new, unseen data.\n",
        "\n",
        "Unsupervised Learning: Unlike supervised learning, unsupervised learning uses data that is not labeled. The goal is to identify patterns or groupings in the data without prior knowledge of the output. Clustering is a common technique in unsupervised learning, where the algorithm groups similar data points together. An example is customer segmentation, where unsupervised learning can group customers with similar purchasing behavior.\n",
        "\n",
        "Reinforcement Learning: Reinforcement learning involves an agent that interacts with an environment and learns by receiving rewards or penalties. The agent takes actions to maximize its cumulative reward. This type of learning is often used in areas such as robotics, game playing, and autonomous vehicles.\n",
        "\n",
        "Applications of Machine Learning\n",
        "\n",
        "Machine learning is used in various fields and industries, from healthcare to finance. Some notable applications include:\n",
        "\n",
        "Healthcare: Machine learning is used to predict disease outbreaks, analyze medical images, and personalize treatments. For example, machine learning models can analyze X-ray images to detect signs of pneumonia or identify tumors in MRI scans.\n",
        "\n",
        "Finance: In finance, machine learning algorithms are used for fraud detection, risk assessment, and algorithmic trading. By analyzing historical data, machine learning models can identify patterns that might indicate fraudulent activity or predict market trends.\n",
        "\n",
        "Natural Language Processing (NLP): NLP is a branch of machine learning that focuses on the interaction between computers and human language. It includes tasks such as sentiment analysis, language translation, and chatbots. For example, AI models like GPT-3 can generate human-like text based on a given prompt.\n",
        "\n",
        "Challenges in Machine Learning\n",
        "\n",
        "Despite its rapid growth and success, machine learning faces several challenges:\n",
        "\n",
        "Data Quality: The quality of the data used to train machine learning models is crucial. Inaccurate, incomplete, or biased data can lead to poor model performance and incorrect predictions.\n",
        "\n",
        "Overfitting: Overfitting occurs when a model performs well on training data but fails to generalize to new, unseen data. This happens when the model is too complex or learns noise in the training data.\n",
        "\n",
        "Ethical Concerns: Machine learning algorithms can perpetuate biases in data, leading to unfair or discriminatory outcomes. Ensuring fairness and transparency in machine learning models is an ongoing challenge.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Machine learning has become an integral part of modern technology, with applications across many domains. While there are challenges to overcome, the potential for machine learning to drive innovation and improve decision-making is immense. As technology advances, the future of machine learning looks promising, with new techniques and applications emerging regularly.\"\"\""
      ],
      "metadata": {
        "id": "hwvIYGtKb81t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Token Splitting\n",
        "\n",
        "from langchain_text_splitters import TokenTextSplitter\n",
        "\n",
        "splitter = TokenTextSplitter(chunk_size = 40, chunk_overlap = 10)\n",
        "chunks = splitter.split_text(text_corpora)"
      ],
      "metadata": {
        "id": "rcpk5zwncKOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting based on ssentences and a chunk_size to maintain semantic importance and context.\n",
        "\n",
        "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "splitter = SentenceTransformersTokenTextSplitter(chunk_size = 50)\n",
        "sentence_chunks = splitter.split_text(text_corpora)"
      ],
      "metadata": {
        "id": "CTlZamASddkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "splitter = SemanticChunker(HuggingFaceEmbeddings())\n",
        "semantic_chunks = splitter.split_text(text_corpora)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4XG_9-RilF9",
        "outputId": "e5ad071a-eb4f-4821-b401-ac11b95fca4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-68-667887843136>:4: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  splitter = SemanticChunker(HuggingFaceEmbeddings())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 60 , chunk_overlap = 20)\n",
        "char_chunks = splitter.split_text(text_corpora)"
      ],
      "metadata": {
        "id": "TRAJTpWKmJyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load HuggingFace Embeddings (using SBERT)\n",
        "embedding_model_name = \"sentence-transformers/paraphrase-MiniLM-L6-v2\"  # High-quality embeddings\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
        "\n",
        "# Initialize HuggingFace text-generation pipeline (e.g., GPT-2)\n",
        "generation_model = pipeline('text-generation', model='gpt2', tokenizer='gpt2')\n",
        "\n",
        "# Example chunks obtained from different chunking methods\n",
        "# Each method gives a different set of chunks\n",
        "chunk_sets = {\n",
        "    \"token-based chunking with overlapping using TokenTextSplitter\":chunks,\n",
        "    \"Tokenizing at a sentence level using SentenceTransformerTokenTextSplitter\": sentence_chunks,\n",
        "    \"Semantic Chunker\": semantic_chunks,\n",
        "    \"character-based chunking using RecursiveCharacterTextSplitter\" : char_chunks\n",
        "}\n",
        "\n",
        "# Step 1: Convert chunks to Document objects for each chunk set\n",
        "document_sets = {\n",
        "    method: [Document(page_content=chunk) for chunk in chunks if isinstance(chunk, str) and chunk.strip()]\n",
        "    for method, chunks in chunk_sets.items()\n",
        "}\n",
        "\n",
        "# Step 2: Create vector stores for each chunk set using HuggingFaceEmbeddings\n",
        "vectorstores = {}\n",
        "for method, documents in document_sets.items():\n",
        "    vectorstore = FAISS.from_documents(documents, hf_embeddings)\n",
        "    vectorstores[method] = vectorstore\n",
        "\n",
        "# Step 3: Define function to retrieve relevant chunk based on cosine similarity for each set\n",
        "def retrieve_relevant_chunks(query, top_k=1, method=\"fixed_length\"):\n",
        "    query_embedding = hf_embeddings.embed_documents(query)\n",
        "    vectorstore = vectorstores[method]\n",
        "\n",
        "    similarities = []\n",
        "    for doc in vectorstore.index:\n",
        "        doc_embedding = doc.embedding  # Assuming FAISS index holds embeddings\n",
        "        similarity_score = cosine_similarity([query_embedding], [doc_embedding])[0][0]\n",
        "        similarities.append((doc.page_content, similarity_score))\n",
        "\n",
        "    # Sort by similarity and return top-k results\n",
        "    sorted_similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
        "    return [doc[0] for doc in sorted_similarities[:top_k]]\n",
        "\n",
        "# Step 4: Define function to generate text with retrieved chunks from different methods\n",
        "def generate_responses(query, top_k=1):\n",
        "    # Retrieve relevant chunks from each chunking method\n",
        "    responses = {}\n",
        "    for method in vectorstores:\n",
        "        relevant_chunks = retrieve_relevant_chunks(query, top_k, method)\n",
        "        context = \" \".join(relevant_chunks)  # Combine retrieved chunks as context\n",
        "\n",
        "        # Generate a response based on the context for the current chunking method\n",
        "        generated_response = generation_model(context, max_length=100, num_return_sequences=1)\n",
        "        responses[method] = generated_response[0]['generated_text']\n",
        "\n",
        "    return responses\n",
        "\n",
        "# Example usage\n",
        "query = \"Tell me about Machine learning.\"\n",
        "responses = generate_responses(query)\n",
        "for method, response in responses.items():\n",
        "    print(f\"Response from {method} chunking:\")\n",
        "    print(response)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "4ri9URqynq05",
        "outputId": "2f703dec-a10a-42d5-eb41-76c42997583a"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'IndexFlatL2' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-98ec499eea69>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Tell me about Machine learning.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mresponses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_responses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Response from {method} chunking:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-98ec499eea69>\u001b[0m in \u001b[0;36mgenerate_responses\u001b[0;34m(query, top_k)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mresponses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvectorstores\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mrelevant_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve_relevant_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_chunks\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Combine retrieved chunks as context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-98ec499eea69>\u001b[0m in \u001b[0;36mretrieve_relevant_chunks\u001b[0;34m(query, top_k, method)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0msimilarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvectorstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mdoc_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m  \u001b[0;31m# Assuming FAISS index holds embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0msimilarity_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_embedding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc_embedding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'IndexFlatL2' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load HuggingFace Embeddings (using SBERT)\n",
        "embedding_model_name = \"sentence-transformers/paraphrase-MiniLM-L6-v2\"  # High-quality embeddings\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
        "\n",
        "# Initialize HuggingFace text-generation pipeline (e.g., GPT-2)\n",
        "generation_model = pipeline('text-generation', model='gpt2', tokenizer='gpt2')\n",
        "\n",
        "# Example chunks obtained from different chunking methods\n",
        "# Each method gives a different set of chunks\n",
        "chunk_sets = {\n",
        "    \"token-based chunking with overlapping using TokenTextSplitter\": chunks,\n",
        "    \"Tokenizing at a sentence level using SentenceTransformerTokenTextSplitter\": sentence_chunks,\n",
        "    \"Semantic Chunker\": semantic_chunks,\n",
        "    \"character-based chunking using RecursiveCharacterTextSplitter\": char_chunks\n",
        "}\n",
        "\n",
        "# Step 1: Convert chunks to Document objects for each chunk set\n",
        "document_sets = {\n",
        "    method: [Document(page_content=chunk) for chunk in chunks if isinstance(chunk, str) and chunk.strip()]\n",
        "    for method, chunks in chunk_sets.items()\n",
        "}\n",
        "\n",
        "# Step 2: Create vector stores for each chunk set using HuggingFaceEmbeddings\n",
        "vectorstores = {}\n",
        "for method, documents in document_sets.items():\n",
        "    vectorstore = FAISS.from_documents(documents, hf_embeddings)\n",
        "    vectorstores[method] = vectorstore\n",
        "\n",
        "# Step 3: Define function to retrieve relevant chunk based on cosine similarity for each set\n",
        "def retrieve_relevant_chunks(query, top_k=1, method=\"fixed_length\"):\n",
        "    query_embedding = hf_embeddings.embed_documents([query])[0]  # Corrected here\n",
        "    vectorstore = vectorstores[method]\n",
        "\n",
        "    # Use FAISS similarity search\n",
        "    results = vectorstore.similarity_search(query, k=top_k)\n",
        "\n",
        "    # Extract the chunks (no similarity score in metadata, just use FAISS results directly)\n",
        "    similarities = [(doc.page_content) for doc in results]\n",
        "\n",
        "    return similarities\n",
        "\n",
        "# Step 4: Define function to generate text with retrieved chunks from different methods\n",
        "def generate_responses(query, top_k=1):\n",
        "    # Retrieve relevant chunks from each chunking method\n",
        "    responses = {}\n",
        "    for method in vectorstores:\n",
        "        relevant_chunks = retrieve_relevant_chunks(query, top_k, method)\n",
        "        context = \" \".join(relevant_chunks)  # Combine retrieved chunks as context\n",
        "\n",
        "        # Generate a response based on the context for the current chunking method\n",
        "        generated_response = generation_model(context, max_length=500, num_return_sequences=1)\n",
        "        responses[method] = generated_response[0]['generated_text']\n",
        "\n",
        "    return responses\n",
        "\n",
        "# Example usage\n",
        "query = \"Tell me about the types of Machine Learning.\"\n",
        "responses = generate_responses(query)\n",
        "print('\\n\\n')\n",
        "for method, response in responses.items():\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print(f\"\\033[1mResponse from {method} chunking:\\033[0m\")\n",
        "    print(response)\n",
        "    print('---------------------------------------------------------------------------------------------------')\n",
        "    print(\"\\n\\n\")\n"
      ],
      "metadata": {
        "id": "2kKyUbtSwEUW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcad1afc-11ee-4220-a8ba-278db85af3e6"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------\n",
            "\u001b[1mResponse from token-based chunking with overlapping using TokenTextSplitter chunking:\u001b[0m\n",
            ", the future of machine learning looks promising, with new techniques and applications emerging regularly.\n",
            "\n",
            "But the future is different in many areas, particularly in data analysis. Most of the great minds who have defined data science today have gone through research and technical education before starting to use machine learning and machine learning for data analysis. Here's what I know for sure: If you want to get started, look at the following four resources (PDF, 4.3MB). This is a good one to start:\n",
            "\n",
            "The Future of Machine Learning by Lawrence Twomey, CTO of Hadoop and author of The Future of Machine Learning, is now available from Amazon. Don.\n",
            "\n",
            "This is The Future: A Data Science History by George C. Marshall, co-founder of IBM and author of the book, The Data Revolution: A Technical Introduction, which is available from Amazon. Don.\n",
            "\n",
            "The Future of Machine Learning: What's Next by Michael S. Dolan, MD, of Oxford University, in his book, The Future of Machine Learning, is now available from Amazon. Don.\n",
            "\n",
            "The Future of Machine Learning for Machine Learning Applications by Matthew Lai, MD, of Columbia University, is now available from Amazon. Don.\n",
            "\n",
            "Other Resources\n",
            "\n",
            "The Future of Machine Learning for Machine Learning Applications (BMS) — This is available at Amazon.com. Read here.\n",
            "\n",
            "Machine Learning for Computer Science: The Science of Data Decisions and the Future of Automated Sensing. The Future has a list of research, and this is not one of them. Look for the other three or so resources in the table below. It's worth noting that, with the exception of the AI and robotics projects, there is not a single single field of technical research that has come close to understanding how to understand and use machine learning. In fact, there are many open science fields where machine learning has been more or less a dominant area of research for a long time, yet there has not been any systematic work on understanding the future direction of machine learning research. The Future is a wonderful resource, but not as powerful as a set of open source technologies like Deep Deep Machine Learning and ImageNet. It is far more focused and practical, but can be hard to write into a piece of paper.\n",
            "\n",
            "In other news:\n",
            "\n",
            "[1] Michael S. Dolan, MS, PhD, has been a researcher and editor in chief of Artificial Intelligence\n",
            "---------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------\n",
            "\u001b[1mResponse from Tokenizing at a sentence level using SentenceTransformerTokenTextSplitter chunking:\u001b[0m\n",
            "machine learning is a subset of artificial intelligence ( ai ) that allows systems to learn and improve from experience without being explicitly programmed. it involves creating algorithms that can identify patterns in data and make decisions or predictions based on that data. the field has seen rapid growth in recent years, with applications ranging from speech recognition to self - driving cars. types of machine learning machine learning can be classified into three main types : supervised learning : in supervised learning, the algorithm is trained on labeled data. the training set includes input - output pairs, where the model learns to map inputs to the correct output. a common example is image classification, where the model is trained with images that are labeled as belonging to specific categories ( e. g., cat, dog, etc. ). the goal is to learn a mapping that can be used to predict labels for new, unseen data. unsupervised learning : unlike supervised learning, unsupervised learning uses data that is not labeled. the goal is to identify patterns or groupings in the data without prior knowledge of the output. clustering is a common technique in unsupervised learning, where the algorithm groups similar data points together. an example is customer segmentation, where unsupervised learning can group customers with similar purchasing behavior. reinforcement learning : reinforcement learning involves an agent that interacts with an environment and learns by receiving rewards or penalties. the agent takes actions to maximize its cumulative reward. this type of learning is often used in areas such as robotics, game playing, and autonomous vehicles. applications of machine learning machine learning is used in various fields and industries, from healthcare to finance. some notable applications include : healthcare : machine learning is used to predict disease outbreaks, analyze medical images, and personalize treatments. for example, machine learning models can analyze x - ray images to detect signs of pneumonia or identify tumors in mri scans. finance : machine learning is used on finance and other fields to help make loans. and many others as well. data processing : machine learning models learn the structure and properties of a data set based on information it can gather on a network, usually using networks of computers. most machine learning algorithms utilize machine learning as the primary means for classification of data, making learning and actionable predictions with a common model, such as classification of human beings on a computer screen. machine learning in the computer world refers to training, prediction, classification, and reinforcement learning. machine learning algorithms can then interact with humans to learn and use data from the underlying data set.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------\n",
            "\u001b[1mResponse from Semantic Chunker chunking:\u001b[0m\n",
            "Machine learning is a subset of artificial intelligence (AI) that allows systems to learn and improve from experience without being explicitly programmed. It involves creating algorithms that can identify patterns in data and make decisions or predictions based on that data. The field has seen rapid growth in recent years, with applications ranging from speech recognition to self-driving cars. Types of Machine Learning\n",
            "\n",
            "Machine learning can be classified into three main types:\n",
            "\n",
            "Supervised Learning: In supervised learning, the algorithm is trained on labeled data. The training set includes input-output pairs, where the model learns to map inputs to the correct output. A common example is image classification, where the model is trained with images that are labeled as belonging to specific categories (e.g., cat, dog, etc.). The goal is to learn a mapping that can be used to predict labels for new, unseen data. Unsupervised Learning: Unlike supervised learning, unsupervised learning uses data that is not labeled. The goal is to identify patterns or groupings in the data without prior knowledge of the output. Clustering is a common technique in unsupervised learning, where the algorithm groups similar data points together. An example is customer segmentation, where unsupervised learning can group customers with similar purchasing behavior. Reinforcement Learning: Reinforcement learning involves an agent that interacts with an environment and learns by receiving rewards or penalties. The agent takes actions to maximize its cumulative reward. This type of learning is often used in areas such as robotics, game playing, and autonomous vehicles. Applications of Machine Learning\n",
            "\n",
            "Machine learning is used in various fields and industries, from healthcare to finance. Some notable applications include:\n",
            "\n",
            "Healthcare: Machine learning is used to predict disease outbreaks, analyze medical images, and personalize treatments. For example, machine learning models can analyze X-ray images to detect signs of pneumonia or identify tumors in MRI scans. Finance: In finance, machine learning algorithms are used for fraud detection, risk assessment, and algorithmic trading. By analyzing historical data, machine learning models can identify patterns that might indicate fraudulent activity or predict market trends. Natural Language Processing (NLP): NLP is a branch of machine learning that focuses on the interaction between computers and human language. It includes tasks such as sentiment analysis, language translation, and chatbots. NLP is commonly used in finance to provide a platform for studying emerging technologies and their applications. For example, some Firms are using AI to help them determine which options\n",
            "---------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------\n",
            "\u001b[1mResponse from character-based chunking using RecursiveCharacterTextSplitter chunking:\u001b[0m\n",
            "Types of Machine Learning\n",
            "\n",
            "Machine learning can be classified into three main types:\n",
            "\n",
            "Machine Learning in JavaScript (also called NLP, NLP+ or NLP-native), or Machine Learning in Machine Learning (LMI)\n",
            "\n",
            "Clinicians\n",
            "\n",
            "Clinicians are more like programmers than AI enthusiasts. They have two goals. The first one is to develop machine learning algorithms for their products and to use them in machine learning. The algorithms they use for these algorithms are generally considered to be the best algorithms they have ever developed. The second goal is to develop algorithms that are more intelligent (i.e. more accurate), but less complex (i.e. less memory or bandwidth intensive). It is also the goal for any programmer to build a highly trained computer program which is not very complex, it may be even easier to use and learn from this, it may be more difficult to understand and learn from the program itself. In computer science there is not a single, obvious and well understood algorithm. There are some special algorithms which are found inside most people's textbooks but usually are not well understood (e.g. the ability to use NLP to develop an AI or learning from an AI), and algorithms which are most useful to people who learn from the code themselves. It is also recommended to work with programmers who develop their own software such as the code for the language which is used for machine learning. Clinician programs have a very limited repertoire (e.g. they should be written for the most common uses of programming) but in general if anyone wants to learn it by itself they should probably start in computer science or a course of study in computer science. The second task is to develop a good knowledge of all possible algorithms. They may only be built to provide that which they want. The first two tasks are the simplest for computers: to learn a machine algorithm, to teach it to the newbie and to learn a new programming interface. The only problem which is an easy one is understanding how to use a computer program. Most people understand the basics of using a computer program through the examples they use.\n",
            "\n",
            "Why do Python programmers learn more from machine learning by coding? Most common questions about machine learning are about finding common parts and building a program which automates the process of learning. The answer is, yes.\n",
            "\n",
            "Python programmers learn more from machine learning, but also from different kinds of data: to use a program to learn and to learn\n",
            "---------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_coherence(chunk_sets):\n",
        "    coherence_scores = {}\n",
        "    for method, chunks in chunk_sets.items():\n",
        "        similarities = []\n",
        "        for i in range(len(chunks) - 1):\n",
        "            chunk1 = hf_embeddings.embed_documents([chunks[i]])[0]\n",
        "            chunk2 = hf_embeddings.embed_documents([chunks[i + 1]])[0]\n",
        "            similarity = cosine_similarity([chunk1], [chunk2])[0][0]\n",
        "            similarities.append(similarity)\n",
        "\n",
        "        avg_similarity = np.mean(similarities)\n",
        "        coherence_scores[method] = avg_similarity\n",
        "\n",
        "    return coherence_scores\n",
        "\n",
        "\n",
        "coherence_results = evaluate_coherence(chunk_sets)\n",
        "\n",
        "print(\"Coherence Scores:\")\n",
        "for method, score in coherence_results.items():\n",
        "    print(f\"{method}: {np.round(score , 4)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPATCoeYxoCP",
        "outputId": "411cb357-1be7-472a-b237-a275662cc53e"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Scores:\n",
            "token-based chunking with overlapping using TokenTextSplitter: 0.5912\n",
            "\n",
            "Tokenizing at a sentence level using SentenceTransformerTokenTextSplitter: 0.6349\n",
            "\n",
            "Semantic Chunker: 0.5939\n",
            "\n",
            "character-based chunking using RecursiveCharacterTextSplitter: 0.4677\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference_response = \"\"\"\n",
        "Machine learning, a subset of AI, has three main types:\n",
        "\n",
        "Supervised Learning: The algorithm learns from labeled data to map inputs to correct outputs, like in image classification.\n",
        "\n",
        "Unsupervised Learning: Works with unlabeled data to find patterns or groupings, such as customer segmentation.\n",
        "\n",
        "Reinforcement Learning: An agent learns by interacting with an environment and receiving rewards or penalties, used in robotics and game playing.\n",
        "\n",
        "These types are key to applications in fields like image recognition, segmentation, and autonomous driving.\n",
        "\"\"\"\n",
        "\n",
        "def evaluate_relevance(chunk_sets, query):\n",
        "    relevance_scores = {}\n",
        "    for method, chunks in chunk_sets.items():\n",
        "        reference_embedding = hf_embeddings.embed_documents([reference_response])[0]\n",
        "        chunk_relevances = []\n",
        "        for chunk in chunks:\n",
        "            chunk_embedding = hf_embeddings.embed_documents([chunk])[0]\n",
        "            similarity = cosine_similarity([reference_embedding], [chunk_embedding])[0][0]\n",
        "            chunk_relevances.append(similarity)\n",
        "\n",
        "        avg_relevance = np.mean(chunk_relevances)\n",
        "        relevance_scores[method] = avg_relevance\n",
        "\n",
        "    return relevance_scores\n",
        "\n",
        "relevance_results = evaluate_relevance(chunk_sets, query)\n",
        "\n",
        "print(\"Relevance Scores:\\n\")\n",
        "for method, score in relevance_results.items():\n",
        "    print(f\"{method}: {np.round(score , 4)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et5XHd9f2Eb9",
        "outputId": "bd662095-a71b-4566-ccf8-9fbfa1a98db7"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevance Scores:\n",
            "\n",
            "token-based chunking with overlapping using TokenTextSplitter: 0.4964\n",
            "\n",
            "Tokenizing at a sentence level using SentenceTransformerTokenTextSplitter: 0.6529\n",
            "\n",
            "Semantic Chunker: 0.5678\n",
            "\n",
            "character-based chunking using RecursiveCharacterTextSplitter: 0.4896\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_context_preservation(chunk_sets):\n",
        "    context_scores = {}\n",
        "    for method, chunks in chunk_sets.items():\n",
        "        similarities = []\n",
        "        for i in range(len(chunks) - 1):\n",
        "            chunk1 = hf_embeddings.embed_documents([chunks[i]])[0]\n",
        "            chunk2 = hf_embeddings.embed_documents([chunks[i + 1]])[0]\n",
        "            similarity = cosine_similarity([chunk1], [chunk2])[0][0]\n",
        "            similarities.append(similarity)\n",
        "\n",
        "        avg_context = np.mean(similarities)\n",
        "        context_scores[method] = avg_context\n",
        "\n",
        "    return context_scores\n",
        "\n",
        "\n",
        "context_results = evaluate_context_preservation(chunk_sets)\n",
        "\n",
        "print(\"Context Preservation Scores:\\n\")\n",
        "for method, score in context_results.items():\n",
        "    print(f\"{method}: {np.round(score,4)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1zNop0224jP",
        "outputId": "26d48b76-cfa4-4a05-9800-1037464822bc"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Preservation Scores:\n",
            "\n",
            "token-based chunking with overlapping using TokenTextSplitter: 0.5912\n",
            "\n",
            "Tokenizing at a sentence level using SentenceTransformerTokenTextSplitter: 0.6349\n",
            "\n",
            "Semantic Chunker: 0.5939\n",
            "\n",
            "character-based chunking using RecursiveCharacterTextSplitter: 0.4677\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textstat\n",
        "\n",
        "def evaluate_readability(chunk_sets):\n",
        "    readability_scores = {}\n",
        "    for method, chunks in chunk_sets.items():\n",
        "        readability_values = []\n",
        "        for chunk in chunks:\n",
        "            score = textstat.flesch_reading_ease(chunk)\n",
        "            readability_values.append(score)\n",
        "\n",
        "        avg_readability = np.mean(readability_values)\n",
        "        readability_scores[method] = avg_readability\n",
        "\n",
        "    return readability_scores\n",
        "\n",
        "readability_results = evaluate_readability(chunk_sets)\n",
        "\n",
        "print(\"Readability Scores:\\n\")\n",
        "for method, score in readability_results.items():\n",
        "    print(f\"{method}: {np.round(score,4)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbcY3xyv3Ucj",
        "outputId": "6240cf9b-be20-4e32-9cdf-1d55567d413f"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Readability Scores:\n",
            "\n",
            "token-based chunking with overlapping using TokenTextSplitter: 43.4129\n",
            "\n",
            "Tokenizing at a sentence level using SentenceTransformerTokenTextSplitter: 41.075\n",
            "\n",
            "Semantic Chunker: 34.11\n",
            "\n",
            "character-based chunking using RecursiveCharacterTextSplitter: 41.025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ju_cXkhW4Q7k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}